<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Distributed Systems | Quick Notes]]></title>
  <link href="http://asquareb.github.io/blog/categories/distributed-systems/atom.xml" rel="self"/>
  <link href="http://asquareb.github.io/"/>
  <updated>2021-02-15T12:05:14-08:00</updated>
  <id>http://asquareb.github.io/</id>
  <author>
    <name><![CDATA[asquareb]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Amazon Aurora Storage]]></title>
    <link href="http://asquareb.github.io/blog/2021/01/10/amazon-aurora-storage/"/>
    <updated>2021-01-10T12:05:19-08:00</updated>
    <id>http://asquareb.github.io/blog/2021/01/10/amazon-aurora-storage</id>
    <content type="html"><![CDATA[<p>Two fundamental concepts enables Amazon Aurora help meet requirements that need to be satisfied by any cloud based database like seamless scalability, high availability, fault tolerance, quick recovery without compromising on performance or increase in maintenance effort.</p>

<ul>
<li>Monotonically increasing Log Sequence Number (LSN) attached to each log record which is written for changes</li>
<li>A multi tenant distributed storage system built for databases to which multiple database instances can be attached. The storage system performs the persistence functions of a traditional database like writing logs to disk, creating and persisting data pages i.e. the custom storage system understands log records and data pages. Also the storage system makes it possible for Aurora to segregate the compute components of databases namely the SQL layer, transaction management and caching from the storage layer</li>
</ul>


<!--more-->


<h2>Storage System</h2>

<p><img src="/images/aurora/Aurora-storage-workflow.png" ALIGN=”center” /></p>

<p>The storage system exposes API to write redo logs which a database instance can use to persist a redo log entries and need to pass in the LSN for the change while doing so. Since this is a network IO, the database instance can make the request to persist log entries as soon as changes are made. This is in contrast to traditional database where log entries are buffered to group them before persisting since it involves disk IO. The storage system persists the log entry to a “hot log” and acknowledges the database instance. This is the only write request made from the database instance to the storage system which reduces the number of IOs by a factor of ~7 per transaction. Data is stored in 10 GB chunks called segments and replicated 6 ways. For availability and recovery, 2 out of 6 copies are made available in an availability zone with a total of 3 AZs where data is stored. The logical group of 6 segments is called the protection group (PG). Storage segments are distributed across storage nodes which are EC2 instances in AWS. Chain of PGs constitutes a database volume which has a one to one relationship to a database instance and the volume can grow as data grows by adding new PGs. The database instance maintains the metadata about the segments, the protection group to which it is part of, the storage nodes which are responsible for the segments along with the data pages and log offsets which is stored in each segment in AWS key value store DynamoDB.</p>

<p>Database instance sends write requests to all the segments for log write and the write is considered successful if 4 out of the 6 storage node acknowledges that the write is successful. The storage system can identify issues with any storage node or segment and when one is found it will add a  new protection group by replacing the segment with issues with a new segment to the members of the existing protection group. The database instance can decide which PG to drop based on how quickly the issue with the old segment is resolved. Since the write is coordinated by the database instance, there is no need to consensus protocols in the storage layer which reduces complexity.</p>

<p><img src="/images/aurora/Aurora-read-replication.png" ALIGN=”center” /></p>

<p>In the background the storage system coalesces log entries, create/update data pages, garbage collect unwanted data pages, perform consistency checks of pages and backup data pages to external storage like S3 for recovery relieving the database instance from these operations. This also allows multiple database instances to be attached to the same storage volume. When there are multiple read instances attached to the same storage volume, the write instance along with sending log write requests to the storage system, it will also send the log entries to the read database instances so that they can keep their buffer caches current. Multiple write instances are enabled by pre-allocating range of LSNs for each instance so that there are no conflicts between the updates made through the various instances. If transactions from two write instances updates the same rows in a table, the transaction for which the 4 out of 6 quorum writes completes first gets committed and the other transaction will fail.</p>

<p>Aurora database instances maintains various consistency points enabled by the monotonically increasing LSN which makes distributed commits and recovery simpler. Segment complete LSN (SCL) is the low watermark below which all the log records has been received. When there are holes in the log records, storage nodes gossip with other nodes in the PG to which is the segment is part of to fill the holes. Protection group complete LSN (PGCL) keeps track of when 4/6 segment SCLs advance in a PG. Volume complete LSN (VCL) tracks the LSN when PGCLs advance at all the PGs. When a recovery happens, the storage system can truncate all the log records with an LSN larger than the VCL can be truncated.</p>

<p>Database can also set a recovery point based transactions. Each database level transactions is broken into multiple mini-transactions (MTR) that need to be performed in order and atomically. The final record in a mini transaction is marked as Consistency Point LSN (CPL). Volume durable LSN (VDL) tracks the CPL which is smaller than or equal to VCL. During recovery, the database establishes the durable point to be the VDL and the storage system truncates all data above that consistency point.</p>

<h2>Database Cloning</h2>

<p>Cloning database will result in the new database pointing to the same logs and data pages of the original database i.e. the clone will much quicker and will be able to perform reads on the data as of that point in time. Creation of new log segments and data pages are done as writes a made through the new database instance. This satisfies the need for reducing cost of creating database clone and the speed in a cloud offering.</p>

<h2>Database Backtracking</h2>

<p>Aurora can be configured to track changes so that state of the database can be moved to a previous point in time quickly for any reasons like data corruption due to incorrect code. Once backtracking is configured, data pages are not garbage collected by the storage system and tracked so that users can go back to a desired point time.</p>

<p><img src="/images/aurora/Aurora-High-Level.png" ALIGN=”center” /></p>

<h2>References</h2>

<ul>
<li><a href="https://www.amazon.science/publications/amazon-aurora-design-considerations-for-high-throughput-cloud-native-relational-databases">Aurora Design Considerations - SIGMOD 17</a></li>
<li><a href="https://www.amazon.science/publications/amazon-aurora-on-avoiding-distributed-consensus-for-i-os-commits-and-membership-changes">Amazon Aurora: On Avoiding Distributed Consensus</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Amazon Dynamo]]></title>
    <link href="http://asquareb.github.io/blog/2016/04/18/amazon-dynamo/"/>
    <updated>2016-04-18T14:45:20-08:00</updated>
    <id>http://asquareb.github.io/blog/2016/04/18/amazon-dynamo</id>
    <content type="html"><![CDATA[<h2>Requirements Dynamo tries to satisfy</h2>

<ul>
<li>Data read and written are identified uniquely by a key</li>
<li>Data size is small and stored as raw bytes that doesn’t require a relational schema</li>
<li>Queries doesn’t span multiple data items i.e. user queries deal with only one row at a time</li>
<li>Use cases that can tolerate weaker consistency for high availability and require no isolation guarantees</li>
<li>Can be deployed on commodity hardware in a trusted environment that doesn’t require authentication or authorization</li>
</ul>


<!--more-->


<ul>
<li>Users will be able to control durability and consistency to make tradeoff between functionality, performance and cost effectiveness. This is in contrast to traditional database where consistency is favored over availability.</li>
</ul>


<h2>Dynamo design considerations</h2>

<ul>
<li>Systems are prone to server and network failures</li>
<li>Availability can be increased by using optimistic replication but need to be able to do conflict resolution and when I.e. whether during reads or writes</li>
<li>Should be an always available datastore</li>
<li>Incremental scalability to handle increase in user needs</li>
<li>Symmetry of node responsibility i.e. no node is special</li>
<li>Decentralization is favored over centralized control</li>
<li>Heterogeneity of the environment should be expoited</li>
</ul>


<h2>Dynamo uses a synthesis of well known techniques to realize its features</h2>

<ul>
<li>Partitioning using consistent hashing (Distributed Hash Table) and replication for scalability and availability</li>
<li>Quorum based and decentralized replica synchronization protocol</li>
<li>Merkle trees used to identify divergence in data stored in different replicas and recovery</li>
<li>Eventual consistency to make it always write available</li>
<li>Hinted handoff i.e. storing writes temporarily when nodes recover from temporary failures</li>
<li>Object versioning for conflict resolution which can be dealt with at the storage or client layer. In Dynamo conflict resolution is done during reads using vector clocks</li>
<li>Gossip based distributed failure detection and membership protocol which eliminates manual intervention to add or remove nodes</li>
<li>Simple key/value user interface to interact with data using the key</li>
</ul>


<h2>Partitioning Algorithm</h2>

<p>Dynamo uses consistent hashing where the output range of a hash function is treated as a fixed circular space or ring. Each node in Dynamo is assigned a random value within this space which represents its position in the ring. Each data item identified by its key is assigned to a node by hashing the key to identify its position in the ring and assigning to the first node in the ring with a position larger than the item’s position. This node is the coordinator node for the key for writes. Also by assigning items to nodes, adding or removing nodes to Dynamo impacts immediate neighbors and other nodes remain unaffected. To reduce non uniform distribution of data and load distribution due to random position assignment of nodes in the has ring , Dynamo uses virtual nodes. Each virtual node is assigned a position in the hash ring and each physical node is assigned multiple virtual nodes. Varying the number of virtual nodes based on physical node capacity, heterogeneity of the environment can be taken into account. If a node becomes unavailable, virtual nodes help in equal dispersement of load across other nodes.</p>

<p><img src="/images/dynamo/Dynamo-Partitioning-Schemes.png" ALIGN=”center” /></p>

<p>The following three partition schemes used and efficiency of load distribution compared</p>

<ul>
<li>Strategy 1: T random tokens per node and partition by token value</li>
<li>Strategy 2: T random tokens per node and equal sized partition</li>
<li>Strategy 3: Q/S tokens per node and equal sized partition where Q is the number of equally sized partition and S the number of nodes</li>
</ul>


<h2>Replication</h2>

<p>The node to which a data item is assigned to is called the coordinator node which not only stores the data locally and also coordinates the replication of data to “N-1” number of nodes where N is configurable. Successive N-1 nodes in the ring after the coordinator node is selected for replication and are called preference list. Since multiple virtual nodes can be assigned to a single physical node, to avoid copies of replicated data is not stores in the same physical node, nodes are skipped to come-up with the preference list.
If any of the nodes in the preference list is not available, another node will be selected to store the replica of the data with the meta data to hint to which the data belongs. When the target node becomes available, the data is delivered to the preferred node. Nodes storing the hinted handoff data can fail before it is replicated to the target node and it will end up replicas being not consistent. To prevent inconsistencies and to be able to recover quickly, Merkle trees on the data stored in each node is maintained and compared regularly. When a difference is found data is replicated in the background to bring back the replications to be insync.</p>

<h2>Data Versioning</h2>

<p>Dynamo uses vector clocks which is a list of (node, counter) pair in order to capture the causality between versions of same object. When multiple versions of data are retrieved during a read operation, if the counter and nodes in the versions are in order i.e. the nodes and counter of the last version contains all the nodes and the largest counter, then all the older versions can be forgotten. But if there are versions where is there is no causal dependency i.e. node in the (node, counter) pair is not the same in two versions then it need to be reconciled. The reconciliation can be done at the client using business logic, at the storage level with last write wins using physical timestamp or by setting the read replica to 1 and write replica to N which will make sure that all the replicas have the same version.</p>

<h2>Reference</h2>

<ul>
<li><a href="https://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf">Amazon Dynamo</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[End-to-end Arguments in System Design]]></title>
    <link href="http://asquareb.github.io/blog/2016/02/07/end-to-end-arguments-in-system-design/"/>
    <updated>2016-02-07T18:41:26-05:00</updated>
    <id>http://asquareb.github.io/blog/2016/02/07/end-to-end-arguments-in-system-design</id>
    <content type="html"><![CDATA[<p><a href="http://web.mit.edu/Saltzer/www/publications/endtoend/endtoend.pdf">This paper</a> presents the design principle regarding placement of functions in computer system design called &ldquo;end-to-end argument&rdquo;.  The argument of the principle is that any application functions implemented at lower levels of a system may be redundant or of little value when compared with the cost of providing them at lower level.  The paper articulates the argument through requirements and examples in distributed systems like reliable data transmission, encryption, duplicate message detection, message sequencing, detecting host crashes, delivery receipts etc.</p>

<!--more-->


<p>Reliable Data Transmission</p>

<ul>
<li>Even if the communication network provides aid in coping with issues in data transmission like data buffering issues, processor or memory issues, loss of packet, host crashes through duplicate copies, timeout and retry, redundancy for error detection, crash recovery etc, at the end the data transfer application need to perform the check of the data transferred to claim it to be successful</li>
<li>This makes the functions in the network layer for reliable data transfer redundant. More over these functions will impact other applications which will be using the network layer but doesn&rsquo;t require all the functions</li>
</ul>


<p>Lower layers can implement function which can improve the performance of the application using it. For e.g. in the case of reliable data transmission</p>

<ul>
<li>The application need to make sure that the data transferred matches the source for e.g. by using checksums. If the checksum at the source and target don&rsquo;t match the data transfer need to be redone</li>
<li>If functions can be included which will cost less but enhances the end to end reliability this will reduce the retries required to have the data data transferred reliably and hence improves the performance</li>
</ul>


<p>Decisions to include functions in lower layers</p>

<ul>
<li>Need to take into account the cost of implementing it at the lower layer and the impact on other applications which may be using it</li>
<li>Need to be made with the understanding that the higher level layers will have much more information than the lower level layers to guarantee a feature/functionality</li>
</ul>


<p>In order to make these decisions i.e. whether to include functions in lower layers or let the application handle it at the end, application requirements of what need to be accomplished need to be well understood.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Scale in Distributed Systems]]></title>
    <link href="http://asquareb.github.io/blog/2016/01/10/scale-in-distributed-systems/"/>
    <updated>2016-01-10T18:51:35-05:00</updated>
    <id>http://asquareb.github.io/blog/2016/01/10/scale-in-distributed-systems</id>
    <content type="html"><![CDATA[<p><a href="http://clifford.neuman.name/papers/pdf/94--_scale-dist-sys-neuman-readings-dcs.pdf">This paper</a> looks at scale and how it affects distributed systems including highlights of how how scale is addressed in existing systems.
A system is said to be <strong>scalable</strong> if it can handle addition of resources and users without suffering noticeable loss in performance or increase in administrative complexity. Scale also affects the way users perceive the systems. For e.g. as the number of objects accessible grows it becomes increasingly difficult to locate the objects of interest.</p>

<!--more-->


<ul>
<li>Definitions

<ul>
<li>A <strong>distributed system</strong> is a collection of computers, connected by a computer network, working together to collectively implement some minimal set of services.</li>
<li>A service or resource is <strong>replicated</strong> when it has multiple logically identical instances appearing on different nodes in a system</li>
<li>A service is <strong>distributed</strong> when it is provided by multiple nodes each capable of handling a subset of the requests for service. A distribution function maps requests to the subset of the nodes that can handle it</li>
<li><strong>Caching</strong> is a temporary form of replication used to save and reuse of query results on nodes. Caching need to use validation techniques to make sure that the data saved are current</li>
</ul>
</li>
<li>Effects of Scale

<ul>
<li>Reliability

<ul>
<li>Systems should not cease to operate just because nodes are unavailable</li>
<li>Reliability can be improved by increasing the autonomy of the nodes and replication</li>
</ul>
</li>
<li>System Load

<ul>
<li>System query load increases with increase in amount of data, nodes, services</li>
<li>Replication, distribution and caching can be used to reduce the number of requests that need to be handled by each node</li>
</ul>
</li>
<li>Administration

<ul>
<li>With increase in number of nodes, administration of users, services and systems becomes complex</li>
<li>Complexity in administration can be reduced by maintaining common information centrally</li>
</ul>
</li>
<li>Heterogeneity

<ul>
<li>With scale nodes part of the system can not only of different hardware but can also run different OS and different versions of OS</li>
<li><strong>Coherence</strong> an approach which expects nodes in a system support a common interface is one which is used to deal with heterogeneity

<ul>
<li>Common instruction set</li>
<li>Common execution abstraction</li>
<li>Support a common set of protocols</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Distributed system components affected by scale

<ul>
<li>Naming and directory services</li>
<li>Authentication</li>
<li>Authorization</li>
<li>Accounting</li>
<li>Communication</li>
<li>Remote Resources</li>
</ul>
</li>
</ul>


<p><em>Scaling of all the components can be improved by replication, distribution and caching</em></p>

<p>Key points to remember while building scalable systems</p>

<ul>
<li>Replication

<ul>
<li>Replication important resources</li>
<li>Distribute the replicas</li>
<li>Use loose consistency</li>
</ul>
</li>
<li>Distribution

<ul>
<li>Distribute across multiple servers</li>
<li>Distribute evenly</li>
<li>Exploit locality</li>
<li>Avoid upper level of hierarchies</li>
</ul>
</li>
<li>Caching

<ul>
<li>Cache frequently accessed data</li>
<li>Consider access patterns when caching

<ul>
<li>amount of data accessed together, read to write ratio, likelihood of conflicts, number of simultaneous users</li>
</ul>
</li>
<li>Cache timeouts</li>
<li>Caching at multiple levels</li>
<li>Look first locally</li>
<li>Data cached extensively must be changed less frequently</li>
</ul>
</li>
<li>Avoid global broadcast</li>
<li>Shed load but not too much: perform computation where it suits better</li>
<li>Support multiple access mechanisms</li>
<li>Keep users in mind</li>
</ul>


<p>Evaluating distributed systems</p>

<ul>
<li>Use of the system

<ul>
<li>Growth of queries as the system grows</li>
<li>Central servers in the system and issues with replication</li>
</ul>
</li>
<li>Data

<ul>
<li>Increase in data and how it increases data maintained in each node in the system</li>
<li>Increase in query time with increase in data size</li>
<li>Data update process and how it scales</li>
<li>Cache data invalidation and query performance</li>
</ul>
</li>
<li>Administration

<ul>
<li>Does the system require a centralized admin system?</li>
<li>Is it practical in the environment in which the system is used?</li>
</ul>
</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Google Bigtable]]></title>
    <link href="http://asquareb.github.io/blog/2012/02/21/google-bigtable/"/>
    <updated>2012-02-21T21:54:36-08:00</updated>
    <id>http://asquareb.github.io/blog/2012/02/21/google-bigtable</id>
    <content type="html"><![CDATA[<p>Bigtable is Google’s distributed key value data storage system which can scale thousands of machines storing petabytes of data.</p>

<ul>
<li>Provides a simple data model that supports dynamic control over data layouts and formats unlike relational data model</li>
<li>Allows clients to reason about the locality properties of the data represented in the underlying storage</li>
<li>Data in indexed by row and column names that can be arbitrary strings</li>
</ul>


<!--more-->


<ul>
<li>Data is treated as a uninterpreted strings but clients can serialize data in various forms into these strings</li>
<li>Its schema lets clients to dynamically control whether the data need to be served out of disk or memory</li>
</ul>


<h2>Data Model</h2>

<p>Data is stored as a multi-dimensional sorted map which is sparsely populated and distributed across multiple nodes. The map is indexed by row key, column name and timestamp and value in the map is an uninterpreted array of bytes.</p>

<p><img src="/images/bigtable/bigtable-datamodel.png" ALIGN=”center” /></p>

<p>Row keys are arbitrary strings and every read/write operation is atomic regardless of the number of columns of the row involved in the operation. Rows are stored in lexicographical order by row keys are partitioned by row range dynamically. Each row range is called a tablet and is the unit of distribution and load balancing. Client operation on a range of rows will involve only a small set of machines where the tablets for the range is stored.</p>

<p>Column keys are grouped into sets called column families which forms the basic unit for access control and it need to be created before storing any column value. The column key is named using the syntax “family name:column name” and all the columns in a column family is compressed together when enabled. The number of columns in a table is unbounded but the number of columns in a column family can be in the hundreds.</p>

<p>Each cell which is qualified by “rowkey:column family:column name” can have multiple versions and these versions are indexed by timestamp which are 64 bit integers. The timestamps can be assigned by the client or Bigtable assigns the real time in microseconds and cells are stored in decreasing timestamp order so that most recent version is read first. Users can set the number of versions to be stored per column family and Bigtable will perform automatic garbage collections to prune the table to store only the required number of versions.</p>

<p>Bigtable also provides APIs for applications to read, write and scanning of data over a key range. Scans allows clients to limit data by selecting only a subset of column families, limiting the number of columns and also the timestamp. The APIs also support single row transactions which can be used to perform read-modify-write sequences on a single row and also supports cells to be used as integer counters. Similar to stored procedures in relational databases, Bigtable also allows client supplied scripts written in Sawzall to be executed in its address space to perform filtering, transformations and operations on data but does not allow writing back to the tables.</p>

<h2>Internals</h2>

<p>Google’s SSTable file format is used to store data on file and Bigtable leverages Google Filesystem (GFS) as its file storage which provides replication for availability and fault tolerance. Each SSTable contains a sequence of blocks which is 64 KB by default and a block index is stored at the end of SSTable which is used to locate blocks. The block index is loaded into memory when a SSTable is opened. For a read request once the block is identified from the block index then a single disk seek is used to retrieve the data from the block. Optionally the complete SSTable can be configured to be completely mapped into memory which will make the data to be served completely from memory. Each tablet comprises of multiple SSTable files</p>

<p>Bigtable consists of a master server which assigns tablets to multiple tablet servers. Apart from assigning tablets to tablet servers, master is also responsible for expiration of tablet servers, load balancing tablet servers, garbage collection of files in GFS and managing table metadata. Each tablet server manages a set of tablets, handles read/write requests to tablets and splits tablets when they grow large.</p>

<p><img src="/images/bigtable/tablet-hierarchy.png" ALIGN=”center” /></p>

<p>Chubby which is a distributed lock service in Google is used to make sure that there is only one active master at any time. It is also used to discover tablet servers and finalize tablet server deaths. Table schema information is also stored in Chubby along with the access control lists.</p>

<p>Chubby stores the location of root tablet which contains location of all tablets in a special METADATA table. Each METADATA tablet contains the location of a set of user tablets and the root tablet is just the first tablet in the METADATA table, but is never split. The METADATA table stores the location of a tablet under a row key that is an encoding of the tablet’s table identifier and its end row. Clients cache the tablet locations and it gets refreshed when data gets stale.</p>

<p>When a tablet server starts it acquires a lock in Chubby and the master requests for the lock status periodically. A lock gets released when a tablet server is not able to reach Chubby due to a network partition or machine failure. Master is informed about the loss of the lock and it tries to reach Chubby to make sure that the service is alive. If it finds the Chubby service to be alive it takes out the tablet servers from its list of tablet servers that serves data and distributes tablets stored in the failed tablet server to other tablet servers. If in case the tablet server connectivity to Chubby service is restored and tries to acquire the previously held lock, it will fail and it will shutdown itself to restart and create a new lock.</p>

<p>Master also takes an exclusive lock in Chubby when it starts and if the session with Chubby fails, master will kill itself. The exclusive lock makes sure that there is only one Master alive at a time. When a new master service comes up, it scans Chubby to find all the live tablet servers and communicates with them to discover all the tablets stored in them. It also scans the METADATA table to identify any tablets which is not assigned to assign them.</p>

<p>During a write request, the client identifies the tablet and the tablet server to which the data need to be stored and send the request to the tablet server. The tablet server writes to a tablet log for recovery and also to an in-memory sorted buffer called memtable. When the buffer fills up it is flushed to disk and stored as SSTables. On read requests, the data for the SSTables and any latest data the in-memory buffer is merged to respond to the request. In order to minimize the number of SSTable files, a subset of these files are periodically merged to combine them into a single SSTable and the process is called merging compaction.  A major compaction is run to merge all the SSTables into a single SSTable and during this process any deleted rows is also removed from the files. A delete request is marked by an additional row identifying the deletion and the deleted rows are not physically removed until major compaction.</p>

<p>Clients can group multiple columns families together into a locality group which will result in a separate SSTable generated for each locality group in each tablet. By grouping commonly accessed column families will result in better read efficiency. Clients can also enable compression for a locality group and the type of compression to be used which will be applied to each block in a SSTable.</p>

<p>To improve read performance, tablet servers uses two levels of caching. The block cache is the lower level cache used to cache data read from GFS and the scan cache is the higher level cache which stores the key-value pairs returned by the SSTable interface to the tablet server code. Block cache improves performance of queries that tend to read data which is close to the data which was recently read while scan cache helps queries that tend to read the same data repeatedly. Bloomfilters which can identity all SSTables in which the requested data is not available is used to help reduce the number of SSTables that need to be read that in-turn improves the read performance.</p>

<h2>Reference</h2>

<ul>
<li><a href="https://research.google/pubs/pub27898/">Google Bigtable</a></li>
</ul>

]]></content>
  </entry>
  
</feed>
