<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Paper | Quick Notes]]></title>
  <link href="http://asquareb.github.io/blog/categories/paper/atom.xml" rel="self"/>
  <link href="http://asquareb.github.io/"/>
  <updated>2022-01-10T22:45:41-08:00</updated>
  <id>http://asquareb.github.io/</id>
  <author>
    <name><![CDATA[asquareb]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Amazon Aurora Storage]]></title>
    <link href="http://asquareb.github.io/blog/2021/01/10/amazon-aurora-storage/"/>
    <updated>2021-01-10T12:05:19-08:00</updated>
    <id>http://asquareb.github.io/blog/2021/01/10/amazon-aurora-storage</id>
    <content type="html"><![CDATA[<p>Two fundamental concepts enables Amazon Aurora help meet requirements that need to be satisfied by any cloud based database like seamless scalability, high availability, fault tolerance, quick recovery without compromising on performance or increase in maintenance effort.</p>

<ul>
<li>Monotonically increasing Log Sequence Number (LSN) attached to each log record which is written for changes</li>
<li>A multi tenant distributed storage system built for databases to which multiple database instances can be attached. The storage system performs the persistence functions of a traditional database like writing logs to disk, creating and persisting data pages i.e. the custom storage system understands log records and data pages. Also the storage system makes it possible for Aurora to segregate the compute components of databases namely the SQL layer, transaction management and caching from the storage layer</li>
</ul>


<!--more-->


<h2>Storage System</h2>

<p><img src="/images/aurora/Aurora-storage-workflow.png" ALIGN=”center” /></p>

<p>The storage system exposes API to write redo logs which a database instance can use to persist a redo log entries and need to pass in the LSN for the change while doing so. Since this is a network IO, the database instance can make the request to persist log entries as soon as changes are made. This is in contrast to traditional database where log entries are buffered to group them before persisting since it involves disk IO. The storage system persists the log entry to a “hot log” and acknowledges the database instance. This is the only write request made from the database instance to the storage system which reduces the number of IOs by a factor of ~7 per transaction. Data is stored in 10 GB chunks called segments and replicated 6 ways. For availability and recovery, 2 out of 6 copies are made available in an availability zone with a total of 3 AZs where data is stored. The logical group of 6 segments is called the protection group (PG). Storage segments are distributed across storage nodes which are EC2 instances in AWS. Chain of PGs constitutes a database volume which has a one to one relationship to a database instance and the volume can grow as data grows by adding new PGs. The database instance maintains the metadata about the segments, the protection group to which it is part of, the storage nodes which are responsible for the segments along with the data pages and log offsets which is stored in each segment in AWS key value store DynamoDB.</p>

<p>Database instance sends write requests to all the segments for log write and the write is considered successful if 4 out of the 6 storage node acknowledges that the write is successful. The storage system can identify issues with any storage node or segment and when one is found it will add a  new protection group by replacing the segment with issues with a new segment to the members of the existing protection group. The database instance can decide which PG to drop based on how quickly the issue with the old segment is resolved. Since the write is coordinated by the database instance, there is no need to consensus protocols in the storage layer which reduces complexity.</p>

<p><img src="/images/aurora/Aurora-read-replication.png" ALIGN=”center” /></p>

<p>In the background the storage system coalesces log entries, create/update data pages, garbage collect unwanted data pages, perform consistency checks of pages and backup data pages to external storage like S3 for recovery relieving the database instance from these operations. This also allows multiple database instances to be attached to the same storage volume. When there are multiple read instances attached to the same storage volume, the write instance along with sending log write requests to the storage system, it will also send the log entries to the read database instances so that they can keep their buffer caches current. Multiple write instances are enabled by pre-allocating range of LSNs for each instance so that there are no conflicts between the updates made through the various instances. If transactions from two write instances updates the same rows in a table, the transaction for which the 4 out of 6 quorum writes completes first gets committed and the other transaction will fail.</p>

<p>Aurora database instances maintains various consistency points enabled by the monotonically increasing LSN which makes distributed commits and recovery simpler. Segment complete LSN (SCL) is the low watermark below which all the log records has been received. When there are holes in the log records, storage nodes gossip with other nodes in the PG to which is the segment is part of to fill the holes. Protection group complete LSN (PGCL) keeps track of when 4/6 segment SCLs advance in a PG. Volume complete LSN (VCL) tracks the LSN when PGCLs advance at all the PGs. When a recovery happens, the storage system can truncate all the log records with an LSN larger than the VCL can be truncated.</p>

<p>Database can also set a recovery point based transactions. Each database level transactions is broken into multiple mini-transactions (MTR) that need to be performed in order and atomically. The final record in a mini transaction is marked as Consistency Point LSN (CPL). Volume durable LSN (VDL) tracks the CPL which is smaller than or equal to VCL. During recovery, the database establishes the durable point to be the VDL and the storage system truncates all data above that consistency point.</p>

<h2>Database Cloning</h2>

<p>Cloning database will result in the new database pointing to the same logs and data pages of the original database i.e. the clone will much quicker and will be able to perform reads on the data as of that point in time. Creation of new log segments and data pages are done as writes a made through the new database instance. This satisfies the need for reducing cost of creating database clone and the speed in a cloud offering.</p>

<h2>Database Backtracking</h2>

<p>Aurora can be configured to track changes so that state of the database can be moved to a previous point in time quickly for any reasons like data corruption due to incorrect code. Once backtracking is configured, data pages are not garbage collected by the storage system and tracked so that users can go back to a desired point time.</p>

<p><img src="/images/aurora/Aurora-High-Level.png" ALIGN=”center” /></p>

<h2>References</h2>

<ul>
<li><a href="https://www.amazon.science/publications/amazon-aurora-design-considerations-for-high-throughput-cloud-native-relational-databases">Aurora Design Considerations - SIGMOD 17</a></li>
<li><a href="https://www.amazon.science/publications/amazon-aurora-on-avoiding-distributed-consensus-for-i-os-commits-and-membership-changes">Amazon Aurora: On Avoiding Distributed Consensus</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Active DBMS]]></title>
    <link href="http://asquareb.github.io/blog/2020/02/15/active-dbms/"/>
    <updated>2020-02-15T11:55:07-08:00</updated>
    <id>http://asquareb.github.io/blog/2020/02/15/active-dbms</id>
    <content type="html"><![CDATA[<p>Passive database management systems (DBMS) are program driven i.e. users query the current state of database and retrieve the information currently available in the database.  An active database is one which automatically executes user specified actions when specified condition arise. The <a href="#reference">first paper</a> details an architecture for an active database using Event-Condition-Action (ECA) rules as a formalism for active database capabilities. The <a href="#reference">second paper</a> details an architecture of transforming a passive DBMS to an active DBMS.</p>

<!--more-->


<p>Fundamentally in an active database, users can create a rule which defines the conditions that need to be met and what action need to be taken by the database. When a DML statement is executed on data in the DBMS, it is checked to see whether the DML caused any of the conditions to be satisfied and if so the action is taken by the DBMS. Anyone who is familiar to triggers in DBMS which was just getting supported in commercial DBMS when these papers where published can relate to this ECA paradigm.</p>

<p>At a high level, the following functional components are required for an active DBMS in addition to code data management and transaction management functionality.</p>

<p><img src="/images/streaming/active-dbms-functional-components.png" ALIGN=”center” /></p>

<ul>
<li><em>Condition evaluator</em> which evaluates rule conditions against events to check whether they satisfy the conditions and informs the rule manager</li>
<li><em>Rule manager</em> manages the definition of rules, takes required action when conditions of rules are met by any event and also interacts with the transaction manager to couple the actions with the transaction initiated by the event</li>
<li><em>Event detectors</em> identifies any DML operation and informs the rule manager</li>
</ul>


<h2>Altert</h2>

<p><em>Alert</em> follows evolutionary approach of extending a passive DBMS into an active DBMS and the components implemented for the extension provides insights into the basic components used in the current data data streaming and processing technologies like Apache Kafka.</p>

<p><img src="/images/streaming/alert-architecture.png" ALIGN=”center” /></p>

<p>It introduces the notion of <em>active tables</em> and <em>active queries</em> . Active tables are append-only tables in which the tuples are never updated in-place and new tuples are added at the end and active queries are queries that range over the tables. When a cursor is opened for an active query involving one more active tables, tuples added to an active table after the cursor was opened also contribute to answer tuples. This the active queries are defined over past, present and future data where as the domain of the passive queries is limited to past and present data. In order to support active queries a new SQL primitive <em>fetch-wait</em> to iterate over active queries was introduced in Alert which blocks when the current answer set is exhausted and resumes returning data when new tuple is available. The active tables are defined by users similar to any passive tables and they are data is stored in active tables akin to journals created by many applications such as banking transactions.</p>

<p>Users can create rules using standard SQL which includes the condition which need to satisfied and the action the database need to take when the condition is satisfied by a database event.</p>

<pre><code>Create rule cost_watch as
    SELECT sbmail(‘Irv’,ename)
    FROM journal
    WHERE method_name == ‘expense_claim’
        AND expense_amount &gt; 1000
</code></pre>

<p>Creates a rule to send an email when the conditions are met. Like database views, rules can be referred in any other query. After creating the rules it can be activated and deactivated explicitly. When activating users can specify the transaction and time coupling along with the assertion mode of the rule. Transaction coupling specifies whether the triggered action from the rule need to be executed in the transaction which the triggered event is part of or seperately.  Time coupling specifies whether the triggered action need to be executed synchronously with the triggering event or in parallel with the triggering event asynchrounously.  The assertion mode provides the users the option to specify whether the rule is triggered as soon as the rule condition is satisfied or deferred till the end of the transaction. The following diagram shows the message flow when an event happens which affects an active table which in turn satisfies conditions in multiple rules.</p>

<p><img src="/images/streaming/alert-message.png" ALIGN=”center” /></p>

<p>The alert rule system which is a new component added to extend a passive DBMS to make it active, does any conflict resolutions between multiple rules and identifies the order of execution. Then the rules are executed in the order to fetch the tuples and passed to the associated action in the rules. To reduce the amount of locks taken on data pages when data is read and rules are evaluated, latches are taken on the pages and locks are deferred to the end.</p>

<h2>Reference</h2>

<ul>
<li><a href="https://web.eecs.umich.edu/~jag/eecs584/papers/md89.pdf">The Architecture of An Active Database Management System</a></li>
<li><a href="http://www.vldb.org/conf/1991/P469.PDF">Alert: An Architecture for Transforming a Passive DBMS into an Active DBMS</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The RUM Conjecture]]></title>
    <link href="http://asquareb.github.io/blog/2016/05/10/the-rum-conjecture/"/>
    <updated>2016-05-10T07:19:50-08:00</updated>
    <id>http://asquareb.github.io/blog/2016/05/10/the-rum-conjecture</id>
    <content type="html"><![CDATA[<p>Data access methods need to modified or newly invented to adapt with ever changing workload requirements and hardware changes. This paper looks at the challenges in designing new access methods which increasingly needs to be application and hardware aware. The fundamental challenges faced are to minimize a)  Read time - R b) Update cost - U c) memory over head - M and the conjecture made is that when optimizing the read-update-memory (RUM) overheads, optimizing in any two negatively impacts the third. Deciding which overheads to optimize for and to what extend has always been and remains the prominent part of designing access methods.</p>

<!--more-->


<h2>RUM Overheads</h2>

<p>Access methods enable us to read or write base data potentially using auxiliary data such as indexes to provide performance improvement.</p>

<h3>Read Overhead</h3>

<p>RO is the read amplification which is the ratio between the total amount of data read including auxiliary data and base data, divided by the amount of data read.</p>

<h3>Update Overhead</h3>

<p>UO is the write amplification which is the ratio between the size of physical update performed for one logical update divided by the size of the logical update. A logical update can involve multiple physical updates for e.g. update to base data and auxiliary data like indexes</p>

<h3>Memory Overhead</h3>

<p>MO is the space amplification which is the ratio between the space utilized for auxiliary and base data, divided by the space utilized for base data.</p>

<p>The theoretical minimum for overhead is to have the ratio equal to 1.0 which implies that the base data is always read and updated directly and no extra bit of memory is wasted. Achieving these bounds for all the three overheads simultaneously is not possible as there is always price to pay for every optimization. When designing an access method all the three overheads should be minimal but depending on the application workload and available technology they need to prioritized.</p>

<h2>RUM Conjecture</h2>

<p><em>designing access methods that set an upper bound for two of the RUM overheads, leads to a hard lower bound for the third overhead which cannot be further reduced</em></p>

<p>In other words, we can choose which two overheads to prioritize and optimize for and pay the price by having the third overhead greater than the hard lower bound. The following figure shows some popular access methods mapped to three dimensional RUM space projected on a two dimensional plane.</p>

<p><img src="/images/rum/rum-space.png" ALIGN=”center” /></p>

<p> Using the RUM space we can understand the current access methods interms of which overhead they prioritize and can choose the appropriate one that suits the need. It also helps in designing new access methods or a combination of access methods which can tune based on available technology and dynamically adapt to new workloads and hence covering the RUM space in aggregate.</p>

<h2>Reference</h2>

<ul>
<li><a href="https://stratos.seas.harvard.edu/files/stratos/files/rum.pdf">Designing Access Methods: The RUM Conjecture</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Amazon Dynamo]]></title>
    <link href="http://asquareb.github.io/blog/2016/04/18/amazon-dynamo/"/>
    <updated>2016-04-18T14:45:20-08:00</updated>
    <id>http://asquareb.github.io/blog/2016/04/18/amazon-dynamo</id>
    <content type="html"><![CDATA[<h2>Requirements Dynamo tries to satisfy</h2>

<ul>
<li>Data read and written are identified uniquely by a key</li>
<li>Data size is small and stored as raw bytes that doesn’t require a relational schema</li>
<li>Queries doesn’t span multiple data items i.e. user queries deal with only one row at a time</li>
<li>Use cases that can tolerate weaker consistency for high availability and require no isolation guarantees</li>
<li>Can be deployed on commodity hardware in a trusted environment that doesn’t require authentication or authorization</li>
</ul>


<!--more-->


<ul>
<li>Users will be able to control durability and consistency to make tradeoff between functionality, performance and cost effectiveness. This is in contrast to traditional database where consistency is favored over availability.</li>
</ul>


<h2>Dynamo design considerations</h2>

<ul>
<li>Systems are prone to server and network failures</li>
<li>Availability can be increased by using optimistic replication but need to be able to do conflict resolution and when I.e. whether during reads or writes</li>
<li>Should be an always available datastore</li>
<li>Incremental scalability to handle increase in user needs</li>
<li>Symmetry of node responsibility i.e. no node is special</li>
<li>Decentralization is favored over centralized control</li>
<li>Heterogeneity of the environment should be expoited</li>
</ul>


<h2>Dynamo uses a synthesis of well known techniques to realize its features</h2>

<ul>
<li>Partitioning using consistent hashing (Distributed Hash Table) and replication for scalability and availability</li>
<li>Quorum based and decentralized replica synchronization protocol</li>
<li>Merkle trees used to identify divergence in data stored in different replicas and recovery</li>
<li>Eventual consistency to make it always write available</li>
<li>Hinted handoff i.e. storing writes temporarily when nodes recover from temporary failures</li>
<li>Object versioning for conflict resolution which can be dealt with at the storage or client layer. In Dynamo conflict resolution is done during reads using vector clocks</li>
<li>Gossip based distributed failure detection and membership protocol which eliminates manual intervention to add or remove nodes</li>
<li>Simple key/value user interface to interact with data using the key</li>
</ul>


<h2>Partitioning Algorithm</h2>

<p>Dynamo uses consistent hashing where the output range of a hash function is treated as a fixed circular space or ring. Each node in Dynamo is assigned a random value within this space which represents its position in the ring. Each data item identified by its key is assigned to a node by hashing the key to identify its position in the ring and assigning to the first node in the ring with a position larger than the item’s position. This node is the coordinator node for the key for writes. Also by assigning items to nodes, adding or removing nodes to Dynamo impacts immediate neighbors and other nodes remain unaffected. To reduce non uniform distribution of data and load distribution due to random position assignment of nodes in the has ring , Dynamo uses virtual nodes. Each virtual node is assigned a position in the hash ring and each physical node is assigned multiple virtual nodes. Varying the number of virtual nodes based on physical node capacity, heterogeneity of the environment can be taken into account. If a node becomes unavailable, virtual nodes help in equal dispersement of load across other nodes.</p>

<p><img src="/images/dynamo/Dynamo-Partitioning-Schemes.png" ALIGN=”center” /></p>

<p>The following three partition schemes used and efficiency of load distribution compared</p>

<ul>
<li>Strategy 1: T random tokens per node and partition by token value</li>
<li>Strategy 2: T random tokens per node and equal sized partition</li>
<li>Strategy 3: Q/S tokens per node and equal sized partition where Q is the number of equally sized partition and S the number of nodes</li>
</ul>


<h2>Replication</h2>

<p>The node to which a data item is assigned to is called the coordinator node which not only stores the data locally and also coordinates the replication of data to “N-1” number of nodes where N is configurable. Successive N-1 nodes in the ring after the coordinator node is selected for replication and are called preference list. Since multiple virtual nodes can be assigned to a single physical node, to avoid copies of replicated data is not stores in the same physical node, nodes are skipped to come-up with the preference list.
If any of the nodes in the preference list is not available, another node will be selected to store the replica of the data with the meta data to hint to which the data belongs. When the target node becomes available, the data is delivered to the preferred node. Nodes storing the hinted handoff data can fail before it is replicated to the target node and it will end up replicas being not consistent. To prevent inconsistencies and to be able to recover quickly, Merkle trees on the data stored in each node is maintained and compared regularly. When a difference is found data is replicated in the background to bring back the replications to be insync.</p>

<h2>Data Versioning</h2>

<p>Dynamo uses vector clocks which is a list of (node, counter) pair in order to capture the causality between versions of same object. When multiple versions of data are retrieved during a read operation, if the counter and nodes in the versions are in order i.e. the nodes and counter of the last version contains all the nodes and the largest counter, then all the older versions can be forgotten. But if there are versions where is there is no causal dependency i.e. node in the (node, counter) pair is not the same in two versions then it need to be reconciled. The reconciliation can be done at the client using business logic, at the storage level with last write wins using physical timestamp or by setting the read replica to 1 and write replica to N which will make sure that all the replicas have the same version.</p>

<h2>Reference</h2>

<ul>
<li><a href="https://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf">Amazon Dynamo</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Note on Distributed Computing]]></title>
    <link href="http://asquareb.github.io/blog/2016/03/06/a-note-on-distributed-computing/"/>
    <updated>2016-03-06T16:18:22-05:00</updated>
    <id>http://asquareb.github.io/blog/2016/03/06/a-note-on-distributed-computing</id>
    <content type="html"><![CDATA[<p>A distributed system is a collection of independent computers that appears to its users as a single coherent system. <a href="http://theory.stanford.edu/people/jcm/cs358-96/spring-os.ps">This paper</a> argues that the objects in a distributed object oriented system form a single ontological class where all entities can be described by the specification of the set of interfaces of the objects and the semantics of operation is mistaken. This vision of unified objects for distributed systems is centered around the principles that</p>

<!-- more -->


<ul>
<li>There is a single natural object oriented design for a given application, irrespective of context in which the application will be deployed</li>
<li>Failure and performance issues are tied to the implementation of the components of the application and can be left out of initial design</li>
<li>The interface of an object is independent of the context in which the object is used</li>
</ul>


<p>While this view is a natural extension of object oriented design, it doesn&rsquo;t take into account the important issues in distributed systems namely</p>

<ul>
<li>Latency

<ul>
<li>Ignoring the difference between the performance of local and remote invocations can lead to designs whose implementations will have performance problems because of large amount of communication between components that are in different address spaces and on different machines.</li>
</ul>
</li>
<li>Memory access

<ul>
<li>Disparate address spaces both local and remote makes accessing memory locations/objects a bigger challenge</li>
<li>This would require a common component like DSM which would translate memory pointer/object locations</li>
<li>The other option is that the programmer is aware of the local and remote access which breaks transparency</li>
</ul>
</li>
<li>Partial failure and concurrency due to lack of common agent

<ul>
<li>Interfaces of components can be designed as if all are local resulting in unhandled catastrophic failures</li>
<li>Or design the interfaces as if all objects are remote resulting in the undesirable overhead on components which are local</li>
<li>Partial failure also brings up the challenge of bringing back to an acceptable state which is not there in non distributed systems</li>
</ul>
</li>
</ul>


<p>Historically there is a desire to merge programming and computational models of local and remote computing. For e.g. communications protocol development has followed</p>

<ul>
<li>A path where integration with the current programming language is emphasized</li>
<li>The other path where solving the inherent problems in distributed computing is emphasized</li>
</ul>


<p>These two approaches can be used in distributed object oriented systems by accepting the fact that there are irreconcilable differences in local and distributed computing</p>

<ul>
<li>Using IDLs which can be used to define local and remote objects and the IDL compilers which can vary its output based on object location</li>
<li>Objects which are local but on a different address space can be considered as a separate category of object by the IDL compilers</li>
<li>Developing tools like the ones which can identify the interaction between objects so that they can be designed to be local or remote</li>
<li>Programmers being aware of the location of the objects so that they can think about the problems differently</li>
</ul>

]]></content>
  </entry>
  
</feed>
